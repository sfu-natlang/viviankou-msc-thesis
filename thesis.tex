\documentclass{sfuthesis}
\title{Speed versus Accuracy in Neural Sequence Tagging for Natural Language Processing}
\thesistype{Thesis}
\author{Xinxin Kou}
\previousdegrees{%
	B.Sc. (Hons.), Dalhousie University, 2015}
\degree{Master of Science}
\discipline{Computing Science}
\department{School of Computing Science}
\faculty{Faculty of Applied Sciences}
\copyrightyear{2017}
\semester{Fall 2017}
\date{12 September 2017}

\keywords{Natural Language Processing; Sequence Tagging; Neural Networks}

\committee{%
	\chair{Dr.\ Arrvindh\ Shriraman}{Professor}
	\member{Dr.\ Anoop Sarkar}{Senior Supervisor\\Professor}
	\member{Dr.\ Fred Popowich}{Supervisor\\ Professor}
	\member{Dr.\ Jiannan Wang}{ Examiner \\
	Assistant Professor }
}
%   PACKAGES AND CUSTOMIZATIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Add any packages or custom commands you need for your thesis here.
%   You don't need to call the following packages, which are already called in
%   the sfuthesis class file:
%
%   - appendix
%   - etoolbox
%   - fontenc
%   - geometry
%   - lmodern
%   - nowidow
%   - setspace
%   - tocloft
%
%   If you call one of the above packages (or one of their dependencies) with
%   options, you may get a ''Option clash'' LaTeX error. If you get this error,
%   you can fix it by removing your copy of \usepackage and passing the options
%   you need by adding
%
%       \PassOptionsToPackage{<options>}{<package>}
%
%   before \documentclass{sfuthesis}.
%
\usepackage{natbib}
\usepackage{apalike}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
%\usepackage[numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{multirow}
\usepackage{underscore}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{makecell}
\pgfplotsset{width=10cm,compat=1.15}

\newcommand{\quotes}[1]{\textrm{``#1''}}
\newcommand{\ffa}{Feedforward-History}
\newcommand{\bia}{BiLSTM-Char}
\newcommand{\bib}{BiLSTM-CRF}
\newcommand{\ma}{Feedforward-Mention2Vec}
\newcommand{\mb}{BPE-Mention2Vec}
%   FRONTMATTER  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Title page, committee page, copyright declaration, abstract,
%   dedication, acknowledgements, table of contents, etc.
%

\begin{document}

\frontmatter
\maketitle{}
\makecommittee{}


\begin{abstract}
Sequence Tagging, including part of speech tagging, chunking and named entity recognition, is an important task in NLP. Recurrent neural network models such as Bidirectional LSTMs have produced impressive results on sequence tagging. In this work, we first present a Bidirectional LSTM neural network model for sequence tagging tasks. Then we show a simple and fast greedy sequence tagging system using a feedforward neural network. We compare the speed and accuracy between the Bidirectional LSTM model and the greedy feedforward model. In addition, we propose two new models based on Mention2Vec by \cite{stratos2016mention2vec}: \ma{} for named entity recognition and chunking, and \mb{} for part-of-speech tagging. \ma{} predicts tag boundaries and corresponding types separately. \mb{} uses the Byte Pair Encoding algorithm to segment words first and then predicts the part-of-speech tags for the subword spans. We carefully design the experiments to demonstrate the speed and accuracy trade-off in different models. The empirical results reveal that the greedy feedforward model can achieve comparable accuracy and faster speed than recurrent models for sequence tagging, and \ma{} is competitive with the fully structured BiLSTM model for named entity recognition while being more scalable in the number of named entity types.

\end{abstract}

\begin{dedication} % optional

To my beloved families who always support me and encourage me. 

\end{dedication}

\begin{acknowledgements} % optional

I would like to express my profound sense of gratitude to my supervisor Dr.\ Anoop Sarkar for introducing me to this research topic and providing his continuous support and valuable guidance throughout my graduate study. I can not imagine having a better advisor and mentor. In addition, I would like to express my sincere appreciation to Dr.\ Fred Popowich for his useful advice and feedback on this work, and Dr.\ Jiannan Wang for being my defence examiner and reading my thesis.

Thanks to all of my Natural Language Processing lab mates who helped me. I enjoy spending time with them. 


\end{acknowledgements}

\addtoToC{Table of Contents}\tableofcontents\clearpage
\addtoToC{List of Tables}\listoftables\clearpage
\addtoToC{List of Figures}\listoffigures





%   MAIN MATTER  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Start writing your thesis --- or start \include ing chapters --- here.
%

\mainmatter%

\input{tex/introduction}

\input{tex/bilstm}

\input{tex/feedforward}

\input{tex/mention2vec}

\input{tex/discussion}

\chapter{Conclusion and Future Work}

This thesis presents and compares different neural network models for sequence tagging tasks. The empirical results reveal that simple feedforward networks can achieve competitive results while being significantly faster than the BiLSTM networks. The empirical results also demonstrate that Feedforward-Mention2Vec performs well on Chunking and NER, and it is more scalable in the number of named entity types in NER.

\section{Contribution}
In this thesis, we first built the the state-of-the-art model for sequence tagging: BiLSTM-CRF. We ran the BiLSTM model with different configurations on three sequence tagging task: POS, Chunking and NER. 

Then, we implemented the greedy feedforward sequence tagging system. We compared the decoding speed and performance between BiLSTM models and feedforward models. As the experiments show, BiLSTM models are more accurate then feedforward models in general. Since feedforward models have a simpler architecture and fewer parameters, feedforward models are faster then BiLSTM models. Experiments also show that the feedforward models are not strongly dependent on hand engineered features, and the models are able to automatically learn the useful features for making decisions.

In addition to feedforward models and BiLSTM models, we presented Feedforward-Mention2Vec for Chunking and NER which is a combination of feedforward models and Mention2Vec, and BPE-Mention2Vec for POS which is based on BPE and Mention2Vec. Both of these two models are multitask models. Feedforward-Mention2Vec predicts boundaries of tags and types of the tags separately: it uses a feedforward network and CRF for boundary detection, and a BiLSTM for type prediction. BPE-Mention2Vec first segments words using BPE, and predicts the part-of-speech tags for the subword units using a BiLSTM network. Feedforward-Mention2Vec performs slightly worse than the state-of-the-art model (BiLSTM-CRF), but its decoding speed is faster. In NER, as the number of named entity types grows, the decoding time of Feedforward-Mention2Vec grows linearly while the decoding time of BiLSTM-CRF grows quadratically. 

Lastly, we summarized all the experiment results and compared the performance and decoding speed of all the models presented in this thesis. We provided analysis on the speed and accuracy trade-off in different models. Feedforward is the fastest among all models, since it contains no extra features and employs a simple network architecture. Our re-implementation of BiLSTM-CRF achieves near state-of-the-art performance on POS and NER. Feedforward-History performs better on POS then on Chunking and NER with a faster decoding speed than BiLSTM-CRF. Feedforward-Mention2Vec performs slightly worse than BiLSTM-CRF on NER, but its decoding speed is faster and grows linearly in the number of named entity types.

\section{Future Work}
There are several potential extensions of this thesis we would like to work on in the future:

First, even though the main goal of this thesis is to compare different neural network models and reveal the speed vs accuracy trade-off, we would like to improve the performance of our models and minimize the accuracy gap between the our implementations and the state-of-the-art results, especially for Chunking. Since the number of training data in CoNLL 2000 for Chunking is limiting, the performance of Chunking can be improved by increasing the amount of training data by self-training. We can train a model on labeled training, run on lot of unlabeled data and then re-train on combined data sets. The same method can be applied on NER and POS.

Second, we would like to compare our implementations with the off-the-shelf sequence tagging tools, such as spaCy. We can further develop our models into applications with high data throughput. The applications can be used to analyze real time natural language data, such as Twitter, Facebook, Wikipedia, and even the web.

Third, we would like to explore multitasking models which combine POS, Chunking and NER. We can build a neural network tagging system to train and predict POS tags, Chunking tags, and NER tags jointly. The intermediate representations would benefit from considering linguistic hierarchies in the training process.



%   BACK MATTER  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   References and appendices. Appendices come after the bibliography and
%   should be in the order that they are referred to in the text.
%
%   If you include figures, etc. in an appendix, be sure to use
%
%       \caption[]{...}
%
%   to make sure they are not listed in the List of Figures.
%

%\backmatter%
\cleardoublepage
\phantomsection
\addtoToC{Bibliography}
%\bibliographystyle{apacite}
\bibliographystyle{apalike}
\bibliography{references}
	

%\begin{appendices} % optional
%	\chapter{Code}
%\end{appendices}
\end{document}
